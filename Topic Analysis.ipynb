{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pprint\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "from nltk.book import *\n",
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_pickle('../../Cleaned Data/italian_review_551.pkl')\n",
    "sample = all_data['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into different clusters\n",
    "cluster0 = all_data[all_data['label']==0]\n",
    "cluster1 = all_data[all_data['label']==1]\n",
    "cluster2 = all_data[all_data['label']==2]\n",
    "cluster3 = all_data[all_data['label']==3]\n",
    "cluster4 = all_data[all_data['label']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(cluster):\n",
    "    text_list = ''\n",
    "    for item in cluster['review']:\n",
    "        text_list += str(item)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list0 = get_text(cluster0)\n",
    "text_list1 = get_text(cluster1)\n",
    "text_list2 = get_text(cluster2)\n",
    "text_list3 = get_text(cluster3)\n",
    "text_list4 = get_text(cluster4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# 'Good' and 'great' are widely used in comments, because the common comments of resaurants are approximately positive. \n",
    "# So we delete 'good' and 'great'. And other words make no sense, we also delete them\n",
    "Useless_words = ['will','one','great','good','-','really','italian']\n",
    "def LDA_topic(sample,num,num_topics = 5,passes = 10,num_words=8,Useless_words=Useless_words):\n",
    "    sents = sent_tokenize(sample)\n",
    "    for index in range(len(sents)):\n",
    "        sents[index] = sents[index].strip().replace('\\n','').replace(\"\\'\",\"'\")\n",
    "    sentence = '.'.join(sents)\n",
    "    #convert the whole text into words\n",
    "    text = [word for word in sentence.lower().split() if word not in STOPWORDS and word.isalnum and word not in Useless_words]\n",
    "    \n",
    "    #get the unique token in the texts\n",
    "    dictionary = corpora.Dictionary([text])\n",
    "    #(word_id,freq) pairs by sentence,and generate index #bags of words\n",
    "    corpus = [dictionary.doc2bow(txt) for txt in [text]] \n",
    "    \n",
    "    #Set parameters\n",
    "    num_topics = num_topics #The number of topics that should be generated\n",
    "    passes = passes # The num of passes to refine the model\n",
    "    lda = LdaModel(corpus, #after dealing with frequencies\n",
    "                  id2word=dictionary,#id to words\n",
    "                  num_topics=num_topics,#how many topics wanted\n",
    "                  passes=passes) #passes search through\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(lda.print_topics(num_words=num_words))#each topic can assoicate with more than a word #return 8 words\n",
    "    \n",
    "    list_ = sorted(lda.get_document_topics(corpus[0],minimum_probability=0,per_word_topics=False),key=itemgetter(1),reverse=True)\n",
    "    print('For cluster{},the topic is'.format(num),lda.print_topic(topicno=list_[0][0]),'\\n')\n",
    "    \n",
    "    return lda\n",
    "    \n",
    "#     return lda.print_topics(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   0,\n",
      "        '0.000*\"place\" + 0.000*\"food\" + 0.000*\"pizza\" + 0.000*\"service\" + '\n",
      "        '0.000*\"pasta\" + 0.000*\"ordered\" + 0.000*\"back\" + 0.000*\"us\"'),\n",
      "    (   1,\n",
      "        '0.001*\"pizza\" + 0.001*\"food\" + 0.001*\"place\" + 0.001*\"us\" + '\n",
      "        '0.001*\"restaurant\" + 0.000*\"pasta\" + 0.000*\"service\" + 0.000*\"back\"'),\n",
      "    (   2,\n",
      "        '0.008*\"food\" + 0.008*\"place\" + 0.007*\"pizza\" + 0.006*\"pasta\" + '\n",
      "        '0.005*\"restaurant\" + 0.005*\"ordered\" + 0.005*\"service\" + '\n",
      "        '0.004*\"back\"'),\n",
      "    (   3,\n",
      "        '0.001*\"pizza\" + 0.000*\"place\" + 0.000*\"food\" + 0.000*\"restaurant\" + '\n",
      "        '0.000*\"pasta\" + 0.000*\"ordered\" + 0.000*\"back\" + 0.000*\"service\"'),\n",
      "    (   4,\n",
      "        '0.001*\"food\" + 0.000*\"pasta\" + 0.000*\"place\" + 0.000*\"pizza\" + '\n",
      "        '0.000*\"ordered\" + 0.000*\"restaurant\" + 0.000*\"service\" + '\n",
      "        '0.000*\"back\"')]\n",
      "For cluster0,the topic is 0.008*\"food\" + 0.008*\"place\" + 0.007*\"pizza\" + 0.006*\"pasta\" + 0.005*\"restaurant\" + 0.005*\"ordered\" + 0.005*\"service\" + 0.004*\"back\" + 0.004*\"got\" + 0.004*\"us\" \n",
      "\n",
      "[   (   0,\n",
      "        '0.000*\"pizza\" + 0.000*\"place\" + 0.000*\"food\" + 0.000*\"service\" + '\n",
      "        '0.000*\"restaurant\" + 0.000*\"pasta\" + 0.000*\"sauce\" + 0.000*\"ordered\"'),\n",
      "    (   1,\n",
      "        '0.009*\"food\" + 0.008*\"place\" + 0.008*\"pizza\" + 0.005*\"pasta\" + '\n",
      "        '0.005*\"restaurant\" + 0.005*\"service\" + 0.004*\"ordered\" + 0.004*\"us\"'),\n",
      "    (   2,\n",
      "        '0.000*\"food\" + 0.000*\"place\" + 0.000*\"pizza\" + 0.000*\"pasta\" + '\n",
      "        '0.000*\"restaurant\" + 0.000*\"service\" + 0.000*\"go\" + 0.000*\"came\"'),\n",
      "    (   3,\n",
      "        '0.001*\"food\" + 0.001*\"place\" + 0.001*\"pizza\" + 0.001*\"restaurant\" + '\n",
      "        '0.000*\"service\" + 0.000*\"ordered\" + 0.000*\"us\" + 0.000*\"pasta\"'),\n",
      "    (   4,\n",
      "        '0.001*\"place\" + 0.001*\"food\" + 0.000*\"pizza\" + 0.000*\"pasta\" + '\n",
      "        '0.000*\"ordered\" + 0.000*\"go\" + 0.000*\"service\" + 0.000*\"restaurant\"')]\n",
      "For cluster1,the topic is 0.009*\"food\" + 0.008*\"place\" + 0.008*\"pizza\" + 0.005*\"pasta\" + 0.005*\"restaurant\" + 0.005*\"service\" + 0.004*\"ordered\" + 0.004*\"us\" + 0.004*\"back\" + 0.004*\"go\" \n",
      "\n",
      "[   (   0,\n",
      "        '0.010*\"food\" + 0.007*\"place\" + 0.006*\"restaurant\" + 0.006*\"service\" + '\n",
      "        '0.005*\"pasta\" + 0.005*\"us\" + 0.005*\"ordered\" + 0.004*\"pizza\"'),\n",
      "    (   1,\n",
      "        '0.000*\"food\" + 0.000*\"place\" + 0.000*\"pasta\" + 0.000*\"restaurant\" + '\n",
      "        '0.000*\"service\" + 0.000*\"pizza\" + 0.000*\"us\" + 0.000*\"back\"'),\n",
      "    (   2,\n",
      "        '0.001*\"food\" + 0.001*\"restaurant\" + 0.001*\"place\" + 0.001*\"service\" + '\n",
      "        '0.001*\"ordered\" + 0.001*\"pasta\" + 0.001*\"us\" + 0.000*\"got\"'),\n",
      "    (   3,\n",
      "        '0.003*\"food\" + 0.002*\"place\" + 0.002*\"restaurant\" + 0.001*\"pasta\" + '\n",
      "        '0.001*\"service\" + 0.001*\"dinner\" + 0.001*\"ordered\" + 0.001*\"pizza\"'),\n",
      "    (   4,\n",
      "        '0.001*\"food\" + 0.000*\"place\" + 0.000*\"restaurant\" + 0.000*\"ordered\" + '\n",
      "        '0.000*\"us\" + 0.000*\"pasta\" + 0.000*\"service\" + 0.000*\"back\"')]\n",
      "For cluster2,the topic is 0.010*\"food\" + 0.007*\"place\" + 0.006*\"restaurant\" + 0.006*\"service\" + 0.005*\"pasta\" + 0.005*\"us\" + 0.005*\"ordered\" + 0.004*\"pizza\" + 0.004*\"came\" + 0.004*\"back\" \n",
      "\n",
      "[   (   0,\n",
      "        '0.002*\"place\" + 0.002*\"food\" + 0.001*\"restaurant\" + 0.001*\"pasta\" + '\n",
      "        '0.001*\"us\" + 0.001*\"ordered\" + 0.001*\"came\" + 0.001*\"service\"'),\n",
      "    (   1,\n",
      "        '0.001*\"place\" + 0.000*\"food\" + 0.000*\"little\" + 0.000*\"service\" + '\n",
      "        '0.000*\"pasta\" + 0.000*\"pizza\" + 0.000*\"back\" + 0.000*\"restaurant\"'),\n",
      "    (   2,\n",
      "        '0.000*\"food\" + 0.000*\"place\" + 0.000*\"service\" + 0.000*\"pasta\" + '\n",
      "        '0.000*\"us\" + 0.000*\"ordered\" + 0.000*\"pizza\" + 0.000*\"back\"'),\n",
      "    (   3,\n",
      "        '0.009*\"food\" + 0.008*\"place\" + 0.006*\"pasta\" + 0.005*\"restaurant\" + '\n",
      "        '0.005*\"pizza\" + 0.005*\"ordered\" + 0.005*\"service\" + 0.004*\"little\"'),\n",
      "    (   4,\n",
      "        '0.000*\"food\" + 0.000*\"place\" + 0.000*\"restaurant\" + 0.000*\"pasta\" + '\n",
      "        '0.000*\"little\" + 0.000*\"back\" + 0.000*\"pizza\" + 0.000*\"service\"')]\n",
      "For cluster3,the topic is 0.009*\"food\" + 0.008*\"place\" + 0.006*\"pasta\" + 0.005*\"restaurant\" + 0.005*\"pizza\" + 0.005*\"ordered\" + 0.005*\"service\" + 0.004*\"little\" + 0.004*\"us\" + 0.004*\"back\" \n",
      "\n",
      "[   (   0,\n",
      "        '0.000*\"pizza\" + 0.000*\"place\" + 0.000*\"food\" + 0.000*\"service\" + '\n",
      "        '0.000*\"ordered\" + 0.000*\"come\" + 0.000*\"back\" + 0.000*\"definitely\"'),\n",
      "    (   1,\n",
      "        '0.010*\"pizza\" + 0.009*\"place\" + 0.009*\"food\" + 0.005*\"ordered\" + '\n",
      "        '0.005*\"service\" + 0.004*\"pasta\" + 0.004*\"back\" + 0.004*\"time\"'),\n",
      "    (   2,\n",
      "        '0.000*\"place\" + 0.000*\"pizza\" + 0.000*\"food\" + 0.000*\"service\" + '\n",
      "        '0.000*\"pasta\" + 0.000*\"ordered\" + 0.000*\"restaurant\" + 0.000*\"order\"'),\n",
      "    (   3,\n",
      "        '0.001*\"place\" + 0.001*\"food\" + 0.000*\"pizza\" + 0.000*\"pasta\" + '\n",
      "        '0.000*\"ordered\" + 0.000*\"time\" + 0.000*\"service\" + 0.000*\"back\"'),\n",
      "    (   4,\n",
      "        '0.000*\"pizza\" + 0.000*\"food\" + 0.000*\"place\" + 0.000*\"service\" + '\n",
      "        '0.000*\"ordered\" + 0.000*\"time\" + 0.000*\"go\" + 0.000*\"came\"')]\n",
      "For cluster4,the topic is 0.010*\"pizza\" + 0.009*\"place\" + 0.009*\"food\" + 0.005*\"ordered\" + 0.005*\"service\" + 0.004*\"pasta\" + 0.004*\"back\" + 0.004*\"time\" + 0.004*\"restaurant\" + 0.004*\"go\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [text_list0,text_list1,text_list2,text_list3,text_list4]\n",
    "for index in range(len(text_list)):\n",
    "    LDA_topic(sample = text_list[index],num = index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_batch = ['place','food','service','pizza','pasta','restaurant']\n",
    "def get_sent(text_list,word_batch=word_batch):\n",
    "    word_list = []\n",
    "    text1 = sent_tokenize(text_list)\n",
    "    for word in word_batch:\n",
    "        for sent in text1:\n",
    "            if word in sent:\n",
    "                word_list.append(sent)\n",
    "    word_set = set(word_list)\n",
    "    word_str = ' '.join(list(word_set))\n",
    "    return word_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str0 = get_sent(text_list0)\n",
    "text_str1 = get_sent(text_list1)\n",
    "text_str2 = get_sent(text_list2)\n",
    "text_str3 = get_sent(text_list3)\n",
    "text_str4 = get_sent(text_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_neg_words():\n",
    "    def get_words(url):\n",
    "        import requests\n",
    "        words = requests.get(url).content.decode('latin-1')\n",
    "        word_list = words.split('\\n')\n",
    "        index = 0\n",
    "        while index < len(word_list): #do cleaning\n",
    "            word = word_list[index]\n",
    "            if ';' in word or not word:\n",
    "                word_list.pop(index)\n",
    "            else:\n",
    "                index+=1\n",
    "        return word_list\n",
    "\n",
    "    #Get lists of positive and negative words\n",
    "    p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt' #list of positive words\n",
    "    n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "    positive_words = get_words(p_url)#clean the words\n",
    "    negative_words = get_words(n_url)\n",
    "    return positive_words,negative_words\n",
    "\n",
    "positive_words,negative_words = get_pos_neg_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cluster0', 0.05572068457594491, 0.013949948954413089),\n",
       " ('cluster1', 0.057921835081240575, 0.014208962375938518),\n",
       " ('cluster2', 0.058058923857792155, 0.014634756893784993),\n",
       " ('cluster3', 0.05686397160438154, 0.01420574392367884),\n",
       " ('cluster4', 0.05532745955303047, 0.01613440156239597)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_pos_neg_sentiment_analysis(text_list,debug=False):\n",
    "    positive_words,negative_words = get_pos_neg_words()\n",
    "    from nltk import word_tokenize\n",
    "    results = list()\n",
    "    for text in text_list:\n",
    "        cpos = cneg = lpos = lneg = 0\n",
    "        for word in word_tokenize(text[1]):\n",
    "            if word in positive_words:\n",
    "                if debug: \n",
    "                    print(\"Positive\",word)\n",
    "                cpos+=1\n",
    "            if word in negative_words:\n",
    "                if debug:\n",
    "                    print(\"Negative\",word)\n",
    "                cneg+=1\n",
    "        results.append((text[0],cpos/len(word_tokenize(text[1])),cneg/len(word_tokenize(text[1])))) #normlaized by length\n",
    "    return results\n",
    "\n",
    "pos_neg = do_pos_neg_sentiment_analysis([('cluster0',text_str0),('cluster1',text_str1),('cluster2',text_str2),('cluster3',text_str3),('cluster4',text_str4)])\n",
    "pos_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [('cluster0',text_list0),('cluster1',text_list1),('cluster2',text_list2),('cluster3',text_list3),('cluster4',text_list4)]\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Remove unwanted words\n",
    "#As we look at the cloud, we can get rid of words that don't make sense by adding them to this variable\n",
    "DELETE_WORDS = ['place','food','service','pizza','pasta','restaurant']\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ') #delelte the words, replace with none\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "MIN_LENGTH = 3\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1) #if less than the min length then replace the word\n",
    "    return text_string\n",
    "\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 2\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    text_string = remove_words(texts[i][1])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i//2, i%2] \n",
    "    ax.set_title(texts[i][0])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
